import os
import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers.multi_query import MultiQueryRetriever

# ì˜¤í”ˆAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ''

# Streamlit UI ë° ê¸°ì¡´ ì½”ë“œ ìœ ì§€
# PDF File Load
@st.cache_resource
def load_all_pdfs(folder_path):
    all_pages = []
    
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            file_path = os.path.join(folder_path, filename)
            loader = PyPDFLoader(file_path) 
            pages = loader.load_and_split()
            all_pages.extend(pages)  # ëª¨ë“  PDF íŒŒì¼ì˜ í˜ì´ì§€ë“¤ì„ í•©ì¹¨
            
    return all_pages


# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    
    vectorstore = Chroma.from_documents(
        split_docs, 
        OpenAIEmbeddings(model ='text-embedding-3-small'),
        persist_directory = persist_directory
    )
    
    return vectorstore


# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        
        return Chroma(
            persist_directory = persist_directory,
            embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')
        )
    else:
        return create_vector_store(_docs)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# Initialize the LangChain components
@st.cache_resource
def chaining():
    folder_path = r"../data/gathering"  # data í´ë” ê²½ë¡œ
    pages = load_all_pdfs(folder_path)  # ëª¨ë“  PDF íŒŒì¼ ë¡œë“œ
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # Define the answer question prompt
    qa_system_prompt = """
    You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question.
    If you don't know the answer, just say that you don't know.
    Please only use information from the provided documents about ì§‘íšŒ (protests) and provide answers in Korean using respectful language.\ 
    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model="gpt-4o-mini")
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain


# Streamlit UI
# ë¡œê³  ê²½ë¡œ
path_to_logo = "EchoShieldLogo.png"  # ë¡œê³  íŒŒì¼ ìœ„ì¹˜

# Streamlit í—¤ë”ì— ë¡œê³  ì¶”ê°€
col1, col2 = st.columns([1, 8])
with col1:
    st.image("EchoShieldLogo.png", width=200)
with col2:
    st.markdown(
        f"""
        <div style="display: flex; align-items: center; gap: 10px; background-color: #eaf4ff; padding: 1px; border-radius: 8px;">
            <h3 style="color: #003366; font-family: Arial, sans-serif;">ì§‘íšŒ ì‹œìœ„ ì°¸ì—¬ ì‹œë¯¼ì„ ìœ„í•œ ë§¤ë‰´ì–¼</h3>
        </div>
        """,
        unsafe_allow_html=True
    )


# Streamlit ì‚¬ì´ë“œë°” ë””ìì¸
with st.sidebar:
    st.image("civil-rights.jpg", use_container_width=True)
    st.markdown("### Echo Shield")
    
    # ë§¤ë‰´ì–¼ ì†Œê°œ Toggle
    with st.expander("ğŸ’¬ ë§¤ë‰´ì–¼ ì†Œê°œ"):
        st.write(
            """
            ì´ ì„œë¹„ìŠ¤ëŠ” ì§‘íšŒ ë° ì‹œìœ„ ì°¸ì—¬ ì‹œë¯¼ì„ ìœ„í•´ ì„¤ê³„ëœ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìëŠ” ì´ í”Œë«í¼ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
            
            - ì•ˆì „, ê±´ê°•, ì‘ê¸‰ì²˜ì¹˜, í¸ì˜ ì‹œì„¤ ë“± í•„ìš” ì •ë³´ ì œê³µ
            - ì‹œë¯¼ë“¤ì˜ ì§‘íšŒ ì°¸ì—¬ì— ëŒ€í•œ ë¶ˆì•ˆê° í•´ì†Œ ë° ì•ˆì „í•œ ì°¸ì—¬ ì´‰ì§„
            - ë²•ì  ì •ë³´ ê°€ì´ë“œë¥¼ í†µí•œ ì‹œë¯¼ì˜ ê¶Œë¦¬ ë³´ì¥
            
            **Echo Shield**ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤.
            """
        )

    # ì§ˆë¬¸í•˜ê¸° Toggle
    with st.expander("ğŸ” ì§ˆë¬¸í•˜ê¸°"):
        st.write(
            """
            **ì§ˆë¬¸ ì°½**: ìš°ì¸¡ ì§ˆë¬¸ ì°½ì„ ì‚¬ìš©í•˜ì—¬ ì¦‰ê°ì ì¸ ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”.\n
            **E-mail ë¬¸ì˜**: ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ ì‚¬í•­ì€ ì•„ë˜ ì´ë©”ì¼ë¡œ ë³´ë‚´ì£¼ì„¸ìš”.\n
            ğŸ“§ **aiffelds@gmail.com**
            """
        )

    # ê¸°íƒ€ ì •ë³´ Toggle
    with st.expander("ğŸ“œ ê¸°íƒ€ ì •ë³´"):
        st.write(
            """
            í˜„ì¬ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ê³§ ì—…ë°ì´íŠ¸ë  ì˜ˆì •ì´ë‹ˆ ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤!
            """
        )

rag_chain = chaining()

# Display messages from session state
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§‘íšŒ ì¤‘ ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹ ê°€ìš”?"}]


if st.button("ëŒ€í™” ê¸°ë¡ ì œê±°"): # ì„¸ì…˜ ì´ˆê¸°í™”
    st.session_state["messages"] = [{"role": "assistant", "content": "ì§‘íšŒ ì¤‘ ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹ ê°€ìš”?"}]


for msg in st.session_state.messages:
    st.chat_message(msg['role']).write(msg['content'])


# Handle user input and generate response
if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.chat_message("human").write(prompt_message)
    st.session_state.messages.append({"role": "user", "content": prompt_message})
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            response = rag_chain.invoke(prompt_message)
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.write(response)
